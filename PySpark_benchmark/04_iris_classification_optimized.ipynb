{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized PySpark ML pipeline for classification (Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PYSPARK OPTIMIZED BENCHMARK - IRIS DATASET\n",
      "======================================================================\n",
      "Optimizations: Kryo Serializer, Explicit Schema, Caching, Tuned Partitions\n",
      "\n",
      "Initializing Spark Session (with optimizations)...\n",
      "   Initialization time: 0.15s\n",
      "\n",
      "Loading data with explicit schema from ../datasets/iris.csv...\n",
      "   Loaded 150 records in 0.10s\n",
      "\n",
      "Preprocessing & caching data...\n",
      "   Training size: 104, Test size: 46\n",
      "   Preprocessing + Caching time: 0.30s\n",
      "\n",
      "Training and evaluating models...\n",
      "\n",
      "   Training: Logistic Regression...\n",
      "      [Logistic Regression]\n",
      "      Accuracy:   0.9783\n",
      "      F1 Score:   0.9785\n",
      "      Time:       0.49s\n",
      "      Throughput: 211 records/s\n",
      "\n",
      "   Training: Decision Tree...\n",
      "      [Decision Tree]\n",
      "      Accuracy:   0.9783\n",
      "      F1 Score:   0.9785\n",
      "      Time:       0.37s\n",
      "      Throughput: 281 records/s\n",
      "\n",
      "Cleaning up...\n",
      "   Cleanup time: 0.00s\n",
      "\n",
      "======================================================================\n",
      "TIMING BREAKDOWN\n",
      "======================================================================\n",
      "Spark Initialization:       0.15s\n",
      "Data Loading:               0.10s\n",
      "Preprocessing+Cache:        0.30s\n",
      "Training (both models):     0.87s\n",
      "Cleanup:                    0.00s\n",
      "----------------------------------------------------------------------\n",
      "TOTAL END-TO-END TIME:      1.42s\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "--- Summary of OPTIMIZED Benchmark (Iris) ---\n",
      "======================================================================\n",
      "Model                | Acc    | F1     | Time (s)  | Throughput (rec/s)  \n",
      "----------------------------------------------------------------------\n",
      "Logistic Regression  | 0.9783 | 0.9785 | 0.49s | 211\n",
      "Decision Tree        | 0.9783 | 0.9785 | 0.37s | 281\n",
      "\n",
      "======================================================================\n",
      "COMPARISON METRICS\n",
      "======================================================================\n",
      "Total Job Time:         1.42s\n",
      "Training Records:       104\n",
      "Test Records:           46\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "DATA_PATH = \"../datasets/iris.csv\" \n",
    "MAX_LR_ITERATIONS = 10\n",
    "\n",
    "# ============================================================\n",
    "# START TOTAL BENCHMARK TIMING\n",
    "# ============================================================\n",
    "TOTAL_BENCHMARK_START = time.time()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PYSPARK OPTIMIZED BENCHMARK - IRIS DATASET\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Optimizations: Kryo Serializer, Explicit Schema, Caching, Tuned Partitions\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# INITIALIZE SPARK SESSION (OPTIMIZED)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nInitializing Spark Session (with optimizations)...\")\n",
    "init_start = time.time()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IrisClassificationOptimized\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"24\") \\\n",
    "    .config(\"spark.default.parallelism\", \"24\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "init_time = time.time() - init_start\n",
    "print(f\"   Initialization time: {init_time:.2f}s\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD DATA WITH EXPLICIT SCHEMA\n",
    "# ------------------------------------------------------------\n",
    "print(f\"\\nLoading data with explicit schema from {DATA_PATH}...\")\n",
    "load_start = time.time()\n",
    "\n",
    "# Define schema explicitly (avoids inference pass)\n",
    "schema = StructType([\n",
    "    StructField(\"sepal length (cm)\", DoubleType(), True),\n",
    "    StructField(\"sepal width (cm)\", DoubleType(), True),\n",
    "    StructField(\"petal length (cm)\", DoubleType(), True),\n",
    "    StructField(\"petal width (cm)\", DoubleType(), True),\n",
    "    StructField(\"species\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(DATA_PATH, header=True, schema=schema)\n",
    "total_records = df.count()\n",
    "load_time = time.time() - load_start\n",
    "\n",
    "print(f\"   Loaded {total_records} records in {load_time:.2f}s\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PREPROCESS AND CACHE DATA\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nPreprocessing & caching data...\")\n",
    "prep_start = time.time()\n",
    "\n",
    "feature_columns = df.columns[:-1]\n",
    "label_column = df.columns[-1]\n",
    "\n",
    "# String Indexing\n",
    "indexer = StringIndexer(inputCol=label_column, outputCol=\"label\")\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "# Vector Assembly\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df_indexed).select(\"features\", \"label\")\n",
    "\n",
    "# Split Data\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# OPTIMIZATION: Persist with Memory+Disk\n",
    "train_data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_data.cache()\n",
    "\n",
    "# Force materialization\n",
    "train_count = train_data.count()\n",
    "test_count = test_data.count()\n",
    "\n",
    "prep_time = time.time() - prep_start\n",
    "\n",
    "print(f\"   Training size: {train_count}, Test size: {test_count}\")\n",
    "print(f\"   Preprocessing + Caching time: {prep_time:.2f}s\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TRAIN AND EVALUATE MODELS\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nTraining and evaluating models...\")\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "def train_and_evaluate(model, name, total_train_records):\n",
    "    print(f\"\\n   Training: {name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Train (reads from cache)\n",
    "    model_fit = model.fit(train_data)\n",
    "    predictions = model_fit.transform(test_data)\n",
    "    \n",
    "    # Force execution\n",
    "    predictions.cache()\n",
    "    num_predictions = predictions.count()\n",
    "    \n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    f1_score = f1_evaluator.evaluate(predictions)\n",
    "    \n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    throughput = total_train_records / time_taken if time_taken > 0 else 0\n",
    "    \n",
    "    print(f\"      [{name}]\")\n",
    "    print(f\"      Accuracy:   {accuracy:.4f}\")\n",
    "    print(f\"      F1 Score:   {f1_score:.4f}\")\n",
    "    print(f\"      Time:       {time_taken:.2f}s\")\n",
    "    print(f\"      Throughput: {throughput:,.0f} records/s\")\n",
    "    \n",
    "    predictions.unpersist()\n",
    "    return accuracy, f1_score, time_taken, throughput\n",
    "\n",
    "results = []\n",
    "training_start = time.time()\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(maxIter=MAX_LR_ITERATIONS, featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_acc, lr_f1, lr_time, lr_tput = train_and_evaluate(lr_model, \"Logistic Regression\", train_count)\n",
    "results.append({\"Model\": \"Logistic Regression\", \"Accuracy\": lr_acc, \"F1 Score\": lr_f1, \"Time\": lr_time, \"Throughput\": lr_tput})\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "dt_acc, dt_f1, dt_time, dt_tput = train_and_evaluate(dt_model, \"Decision Tree\", train_count)\n",
    "results.append({\"Model\": \"Decision Tree\", \"Accuracy\": dt_acc, \"F1 Score\": dt_f1, \"Time\": dt_time, \"Throughput\": dt_tput})\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CLEANUP\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nCleaning up...\")\n",
    "cleanup_start = time.time()\n",
    "\n",
    "train_data.unpersist()\n",
    "test_data.unpersist()\n",
    "\n",
    "cleanup_time = time.time() - cleanup_start\n",
    "print(f\"   Cleanup time: {cleanup_time:.2f}s\")\n",
    "\n",
    "# ============================================================\n",
    "# END TOTAL BENCHMARK TIMING\n",
    "# ============================================================\n",
    "TOTAL_BENCHMARK_END = time.time()\n",
    "TOTAL_TIME = TOTAL_BENCHMARK_END - TOTAL_BENCHMARK_START\n",
    "\n",
    "# ============================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TIMING BREAKDOWN\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Spark Initialization:   {init_time:>8.2f}s\")\n",
    "print(f\"Data Loading:           {load_time:>8.2f}s\")\n",
    "print(f\"Preprocessing+Cache:    {prep_time:>8.2f}s\")\n",
    "print(f\"Training (both models): {training_time:>8.2f}s\")\n",
    "print(f\"Cleanup:                {cleanup_time:>8.2f}s\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"TOTAL END-TO-END TIME:  {TOTAL_TIME:>8.2f}s\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"--- Summary of OPTIMIZED Benchmark (Iris) ---\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<20} | {'Acc':<6} | {'F1':<6} | {'Time (s)':<9} | {'Throughput (rec/s)':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for res in results:\n",
    "    print(f\"{res['Model']:<20} | {res['Accuracy']:.4f} | {res['F1 Score']:.4f} | {res['Time']:.2f}s | {res['Throughput']:,.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total Job Time:         {TOTAL_TIME:.2f}s\")\n",
    "print(f\"Training Records:       {train_count}\")\n",
    "print(f\"Test Records:           {test_count}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
